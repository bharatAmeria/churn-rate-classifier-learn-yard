1. Create repo, clone it in local

2. Create a virtual environment -> pyhton3 -m venv venv

3. Activate the virtual environment -> source venv/bin/Activate

4. Create template.py file and copy code and runt it ( it create a project structure for you )

5. Add code to setup.py, project.toml, testEnvironment.py, requirements.txt, src/constants/__init__.py.
    Now run testEnvironment.py (verifies the pyhton environment and install dependecies from requirements.txt)

6. Add code to below files/folders inside src dir:
    - src/logger/__init__.py
    - src/exception/__init__.py
    - src/config/__init__.py
    - src/components/data_ingestion.py
    - src/components/data_processing.py
    - src/config/__init__.py
    - src/components/model.py
    - src/pipeline/stage01_data_ingestion.py
    - src/pipeline/stage02_data_processing.py
    - src/pipeline/stage04_model_training.py
    - config.yaml
    - src/data_fetch/bank_churn_data.py
    - src/connection/mongo_conn.py

7. Refer to readme file for mongoDB setup. Ater setup there is connection url store it in the .env folder (MONGODB_URI)

8. To test the pipeline locally. so the at all components are working correctly. 
   Add to train.py file and run. 
   It should run without showing any error.

9. Go to: https://dagshub.com/dashboard
10. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
11. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
    dagshub.init(repo_owner='your_repo_owner_name', repo_name='youre_repo_name', mlflow=True)

12. pip install dagshub & mlflow

------------------------- Airflow ---------------------------
1. Download Astro CLI to manage airflow. -> brew install astro 
2. Initialize astro -> astro dev init
3. Copy dockerfile, docker-compose file.
4. Copy files in the dags folder for Airflow.
5. Start airflow using -> astro dev start (this will start airflow and PostgreSQL)

------------------------- Google Cloud Platform Setup ---------------------------

1. Open Google Cloud Console
2. Navigate to: Compute Engine > VM instances
3. Search for Compute Engine API and enable It.

4. genrate ssh key and also add to gitignore file so that you do not commit accidently 

    { ssh-keygen -t rsa -f .ssh/google_compute_engine -C bharataameriya }
    
5. enable compute engine api first if not enabled
    { vi google_compute_engine.pub }  this opens google_compute_engine.pub file copy and key follow below step.

    If you want to do it manually.
    now navigate to google console > navigation > compute engine > metadata > ssh keys
    (where we upload the keys) and save the key

    Otherwise
    { gcloud compute os-login ssh-keys add --key-file=.ssh/google_compute_engine.pub }

6. now navigate to google console > navigation > compute engine > metadata> ssh keys
   (where we upload the keys) and save the key

7. Click Create Instance > 
    Region: Choose closest region 
    Machine type: e2-medium or higher
    Boot disk: Ubuntu 22.04
    Allow HTTP & HTTPS traffic
    Firewall: ✅ Allow both

8. gcloud compute scp --recurse ./app INSTANCE_NAME:~/ --zone=ZONE

9. copy the external in to connect our instance -> ssh -i <private key> <username>@<external IP>
   (for eg. -> ssh -i gcp bharataameriya@35.239.6.247)

10. Install Docker on GCP VM    
    sudo apt update
    sudo apt install docker.io -y
    sudo usermod -aG docker $USER
    newgrp docker  # Apply use

    Test Docker:
     docker run hello-world

7. cd app
   docker build -t app .
   docker run -d -p 80:8000 app

8. http://<EXTERNAL_VM_IP>


------------------------- Airflow (In GCP)---------------------------
1. SSH into your GCP VM :- 
gcloud compute ssh <your-vm-name> --zone <your-zone>

2. Install dependencies:
sudo apt update && sudo apt install -y \
    curl \
    git \
    docker.io \
    docker-compose \
    python3 \
    python3-pip

3. Enable Docker for your user:
sudo usermod -aG docker $USER
newgrp docker  # Apply group change

4. Install Astro CLI
curl -sSL https://install.astronomer.io | sudo bash

5. Confirm install:
astro version

6. Create a New Astro Project
mkdir airflow-gcp && cd airflow-gcp
astro dev init

7. This will create a project structure like:
airflow-gcp/
├── dags/
├── Dockerfile
├── include/
├── plugins/
├── requirements.txt
├── packages.txt
├── airflow_settings.yaml

8. Start Airflow Locally
astro dev start

9. Access Airflow UI
If running directly in GCP VM, open Airflow at: http://<VM_EXTERNAL_IP>:8080
(You may need to allow traffic on port 8080 from GCP:)

-> In GCP Console:
    VPC Network > Firewall Rules > Create Rule:
    Name: allow-airflow
    Targets: All instances in the network
    Protocols/Ports: tcp:8080
    Source: 0.0.0.0/0 (or restrict to your IP)


------------------------- Google Cloud Instance Cleanup ---------------------------
1. Delete any Docker Images on your Instance ->
            docker image rm app
            docker system prune -a

2. Delete Instance -> gcloud compute instances delete <instance-id> --zone=YOUR_ZONE --quiet
3. Delete FIREWALL RULE -> gcloud compute firewall-rules delete default-allow-http --quiet
4. SSh Keys -> 
            gcloud compute os-login ssh-keys list
            gcloud compute os-login ssh-keys remove --key <KEY_STRING_OR_FINGERPRINT>

5. Now to Disable Api Nvaigate to APIs enabled and services. Disable all the APIs

6. Check Billing -> 
            gcloud compute instances list
            gcloud compute disks list
            gcloud compute firewall-rules list

7. Now Delete the project if you want to.
