1. Create repo, clone it in local

2. Create a virtual environment -> pyhton3 -m venv venv

3. Activate the virtual environment -> source venv/bin/Activate

4. Create template.py file and copy code and runt it ( it create a project structure for you )

5. Add code to setup.py, project.toml, testEnvironment.py, requirements.txt, src/constants/__init__.py.
    Now run testEnvironment.py (verifies the pyhton environment and install dependecies from requirements.txt)

6. Add code to below files/folders inside src dir:
    - src/logger/__init__.py
    - src/exception/__init__.py
    - src/config/__init__.py
    - src/components/data_ingestion.py
    - src/components/data_processing.py
    - src/config/__init__.py
    - src/components/model.py
    - src/pipeline/stage01_data_ingestion.py
    - src/pipeline/stage02_data_processing.py
    - src/pipeline/stage04_model_training.py
    - config.yaml
    - src/data_fetch/bank_churn_data.py
    - src/connection/mongo_conn.py

7.  Refer to readme file for mongoDB setup. Ater setup tehre is connection url store it in the .env folder (MONGODB_URI)

8. To test the pipeline locally. so the at all components are working correctly. 
   Add to train.py file and run. 
   It should run without showing any error.

------------------------- Airflow ---------------------------
1. Create these folders -> dags, logs, plugins, config
2. Start docker-desktop (check "docker --version" & "docker-compose --version") & add dockerfile

3. Check latest apache airflow releases and accordingly download the docker-compose.yaml file -
                                ***Bash Command***
   curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.0.1/docker-compose.yaml'
                                ***Powershell Command***
   Invoke-WebRequest -Uri 'https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml' -OutFile 'docker-compose.yaml'

4. Windows users -> Instead of following step3, use the provided 'docker-compose.yaml' file 
5. On terminal - "docker-compose up -d"
   This will:
    >> Download all required images
    >> Initialize the database
    >> Create an admin user (username: admin, password: admin)
    >> Start all Airflow services

6. (Wait few minutes before proceeding) Access Airflow Web UI ->
    >> Open your web browser and go to: http://localhost:8080
    >> Login with: Username: admin, Password: admin

7. Common Commands ->
    >> Stop Airflow: docker-compose down
    >> Restart Airflow: docker-compose down && docker-compose up -d
    >> View logs: docker-compose logs -f
    >> Check running containers: docker ps

This setup gives you a complete Airflow environment with:
    >> PostgreSQL as the metadata database
    >> Redis as the message broker
    >> CeleryExecutor for parallel task execution
    >> Web UI, scheduler, and worker services

------------------------- Google Cloud Platform Setup ---------------------------

1. Open Google Cloud Console
2. Navigate to: Compute Engine > VM instances
3. Search for Compute Engine API and enable It.

4. genrate ssh key and also add to gitignore file so that you do not commit accidently 

    { ssh-keygen -t rsa -f .ssh/google_compute_engine -C bharataameriya }
    
5. enable compute enginve api first if not enabled
    { vi google_compute_engine.pub }  this opens google_compute_engine.pub file copy and key follow below step.

    If you want to do it manually.
    now navigate to google console > navigation > compute engine > metadata> ssh keys
    (where we upload the keys) and save the key

    Otherwise
    { gcloud compute os-login ssh-keys add --key-file=.ssh/google_compute_engine.pub }

6. now navigate to google console > navigation > compute engine > metadata> ssh keys
   (where we upload the keys) and save the key

7. Click Create Instance > 
    Region: Choose closest region 
    Machine type: e2-medium or higher
    Boot disk: Ubuntu 22.04
    Allow HTTP & HTTPS traffic
    Firewall: âœ… Allow both

8. gcloud compute scp --recurse ./app INSTANCE_NAME:~/ --zone=ZONE

9. copy the external in to connect our instance -> ssh -i <private key> <username>@<external IP>
   (for eg. -> ssh -i gcp bharataameriya@35.239.6.247)

10. Install Docker on GCP VM    
    sudo apt update
    sudo apt install docker.io -y
    sudo usermod -aG docker $USER
    newgrp docker  # Apply use

    Test Docker:
     docker run hello-world

7. cd app
   docker build -t app .
   docker run -d -p 80:8000 app

8. http://<EXTERNAL_VM_IP>



------------------------- Google Cloud Instance Cleanup ---------------------------
1. Delete any Docker Images on your Instance ->
            docker image rm app
            docker system prune -a

2. Delete Instance -> gcloud compute instances delete <instance-id> --zone=YOUR_ZONE --quiet
3. Delete FIREWALL RULE -> gcloud compute firewall-rules delete default-allow-http --quiet
4. SSh Keys -> 
            gcloud compute os-login ssh-keys list
            gcloud compute os-login ssh-keys remove --key <KEY_STRING_OR_FINGERPRINT>

5. Now to Disable Api Nvaigate to APIs enabled and services. Disable all the APIs

6. Check Billing -> 
            gcloud compute instances list
            gcloud compute disks list
            gcloud compute firewall-rules list

7. Now Delete the project if you want to.




